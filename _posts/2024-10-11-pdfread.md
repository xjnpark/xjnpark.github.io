```python
# 데이터 로드 (예: CSV 파일)
data = pd.read_csv('your_data.csv')
```


```python
# 데이터 확인
print(data.head())
```


```python
### 2단계: 데이터 전처리
import re
from sklearn.model_selection import train_test_split

# 텍스트 정제 함수
def clean_text(text):
    text = re.sub(r'\W', ' ', text)  # 특수 문자 제거
    text = text.lower()  # 소문자 변환
    return text

# 데이터 정제
data['cleaned_text'] = data['text_column'].apply(clean_text)

# 학습 데이터와 테스트 데이터 분리
train_data, test_data = train_test_split(data['cleaned_text'], test_size=0.2, random_state=42)
```


```python
### 3단계: 모델 설계

sLLM 모델 설계. Hugging Face의 Transformers 라이브러리를 사용하여 사전 훈련된 모델을 기반으로 fine-tuning 진행.
```


```python
python
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments

# 모델과 토크나이저 로드
model_name = 'KAI_GPT'  # 사용할 모델 이름
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 데이터셋 준비
train_encodings = tokenizer(train_data.tolist(), truncation=True, padding=True)
test_encodings = tokenizer(test_data.tolist(), truncation=True, padding=True)
```


```python
### 4단계: 데이터셋 클래스 정의
PyTorch의 Dataset 클래스를 사용하여 데이터셋 정의.
```


```python
import torch

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = CustomDataset(train_encodings)
test_dataset = CustomDataset(test_encodings)
```


```python
### 5단계: 모델 학습

Trainer를 설정하고 학습 진행.
```


```python
training_args = TrainingArguments(
    output_dir='./results',          # 출력 디렉토리
    num_train_epochs=3,              # 학습 에폭 수
    per_device_train_batch_size=2,   # 배치 크기
    per_device_eval_batch_size=2,    # 평가 배치 크기
    warmup_steps=500,                 # 워밍업 스텝 수
    weight_decay=0.01,                # 가중치 감소
    logging_dir='./logs',            # 로그 디렉토리
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()
```


```python
### 6단계: 모델 평가
trainer.evaluate()

### 7단계: 모델 저장
model.save_pretrained('./my_sLLM_model')
tokenizer.save_pretrained('./my_sLLM_model')


이 과정은 sLLM 모델을 만드는 기본적인 흐름을 보여줍니다. 실제로는 데이터의 특성, 모델의 복잡성, 하이퍼파라미터 조정 등에 따라 추가적인 작업이 필요할 수 있습니다. 
필요에 따라 각 단계를 조정하여 최적의 결과를 얻으시기 바랍니다.
    
```


```python
# pdf 파일과 보고서 데이터 가져오기

# PDF 파일에서 텍스트를 추출하고 이를 데이터프레임으로 변환:
import PyPDF2
import pandas as pd

# PDF 파일 열기
pdf_file_path = 'PyPDF2_ PDF.pdf'  # PDF 파일 경로를 입력하세요
with open(pdf_file_path, 'rb') as file:
    reader = PyPDF2.PdfReader(file)
    text = ''
    
    # 모든 페이지에서 텍스트 추출
    for page in reader.pages:
        text += page.extract_text() + '\n'

# 텍스트를 데이터프레임으로 변환 (예: 각 줄을 행으로)
data = {'Content': text.splitlines()}
df = pd.DataFrame(data)

# 데이터프레임 출력
print(df)
```

                                                   Content
    0                        PyPDF2: PDF 조작을 위한 최고의 Python
    1                                                라이브러리
    2                                       Naomi Clarkson
    3                              Updated on 2023. 8. 17.
    4    PyPDF2는 Py thon에서 PDF 조작을 위한 강력하고 무료이며 오픈 소스 라...
    ..                                                 ...
    165                                              RATH:
    166                                      autopilot for
    167                                             visual
    168                                         exploraion
    169  Copyright © 2024  Kanaries . All rights r eser...
    
    [170 rows x 1 columns]
    

totalPages = reader.numPages
print(totalPages)


```python
import PyPDF2
import pandas as pd
```


```python
# PDF 파일 열기
pdf_file_path = './데이터시장.pdf'  # PDF 파일 경로를 입력하세요
with open(pdf_file_path, 'rb') as file:
    reader = PyPDF2.PdfReader(file)
    # text = ''
    
    # 모든 페이지에서 텍스트 추출
    for page in reader.pages:
        text += page.extract_text() + '\n'

# 텍스트를 데이터프레임으로 변환 (예: 각 줄을 행으로)
data = {'Content': text.splitlines()}
df = pd.DataFrame(data)

# 데이터프레임 출력
print(df)
```


```python
!pip install pdfplumber
```

    Collecting pdfplumber
      Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)
    Collecting pdfminer.six==20231228 (from pdfplumber)
      Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)
    Requirement already satisfied: Pillow>=9.1 in c:\users\internet\desktop\envs\pjk\lib\site-packages (from pdfplumber) (10.4.0)
    Collecting pypdfium2>=4.18.0 (from pdfplumber)
      Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)
    Requirement already satisfied: charset-normalizer>=2.0.0 in c:\users\internet\desktop\envs\pjk\lib\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.12)
    Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)
      Downloading cryptography-43.0.1-cp37-abi3-win_amd64.whl.metadata (5.4 kB)
    Requirement already satisfied: cffi>=1.12 in c:\users\internet\desktop\envs\pjk\lib\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)
    Requirement already satisfied: pycparser in c:\users\internet\desktop\envs\pjk\lib\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)
    Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)
    Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)
       ---------------------------------------- 0.0/5.6 MB ? eta -:--:--
       --------- ------------------------------ 1.3/5.6 MB 5.2 MB/s eta 0:00:01
       -------------------------- ------------- 3.7/5.6 MB 6.1 MB/s eta 0:00:01
       ---------------------------------------  5.5/5.6 MB 7.5 MB/s eta 0:00:01
       ---------------------------------------  5.5/5.6 MB 7.5 MB/s eta 0:00:01
       ---------------------------------------  5.5/5.6 MB 7.5 MB/s eta 0:00:01
       ---------------------------------------  5.5/5.6 MB 7.5 MB/s eta 0:00:01
       ---------------------------------------  5.5/5.6 MB 7.5 MB/s eta 0:00:01
       ---------------------------------------- 5.6/5.6 MB 2.9 MB/s eta 0:00:00
    Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)
       ---------------------------------------- 0.0/2.9 MB ? eta -:--:--
       ---------------------------------------  2.9/2.9 MB 12.9 MB/s eta 0:00:01
       ---------------------------------------- 2.9/2.9 MB 8.0 MB/s eta 0:00:00
    Downloading cryptography-43.0.1-cp37-abi3-win_amd64.whl (3.1 MB)
       ---------------------------------------- 0.0/3.1 MB ? eta -:--:--
       ------------------------------ --------- 2.4/3.1 MB 12.2 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ------------------------------------- -- 2.9/3.1 MB 12.0 MB/s eta 0:00:01
       ---------------------------------------- 3.1/3.1 MB 1.5 MB/s eta 0:00:00
    Installing collected packages: pypdfium2, cryptography, pdfminer.six, pdfplumber
    Successfully installed cryptography-43.0.1 pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0
    


```python
import pdfplumber

with pdfplumber.open('./데이터시장.pdf') as pdf:
    
    first_page = pdf.pages[0]
    print(first_page.extract_text())
```


```python
from PyPDF2 import PdfReader

def get_pdf_content_lines(pdf_file_path):
    with open(pdf_file_path) as f:
        pdf_reader = PdfFileReader(f)
        for page in pdf_reader.pages: 
            for line in page.extractText().splitlines():
                yield line

for line in get_pdf_content_lines('C:/Users/Internet/Downloads/데이터시장.pdf'):
    print(line)
```
